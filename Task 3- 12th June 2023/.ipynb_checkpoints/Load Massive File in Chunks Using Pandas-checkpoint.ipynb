{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ae2be4",
   "metadata": {},
   "source": [
    "### Pandas in flexible and easy to use open-source data analysis tool build on top of python which makes importing and visualizing data of different formats like .csv, .tsv, .txt and even .db files.\n",
    "\n",
    "The method used to read CSV files is read_csv() \n",
    "\n",
    "### Parameters:\n",
    "\n",
    "#### filepath_or_bufferstr : \n",
    "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv.\n",
    "\n",
    "#### iteratorbool : \n",
    "default False Return TextFileReader object for iteration or getting chunks with get_chunk().\n",
    "\n",
    "#### chunksize :\n",
    "int, optional Return TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bc6f6",
   "metadata": {},
   "source": [
    "#### The read_csv() method has many parameters but the one we are interested is chunksize. Technically the number of rows read at a time in a file by pandas is referred to as chunksize. Suppose If the chunksize is 100 then pandas will load the first 100 rows. The object returned is not a data frame but a TextFileReader which needs to be iterated to get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600779b",
   "metadata": {},
   "source": [
    "## Example 1: Loading massive amount of data normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9484757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Island', 'CulmenLength', 'CulmenDepth', 'FlipperLength', 'BodyMass',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "df = pd.read_csv('C:/Users/Pranay/Downloads/penguins.csv')\n",
    "\n",
    "df.columns              # to check the columns of datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc96efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Island         344 non-null    object \n",
      " 1   CulmenLength   342 non-null    float64\n",
      " 2   CulmenDepth    342 non-null    float64\n",
      " 3   FlipperLength  342 non-null    float64\n",
      " 4   BodyMass       342 non-null    float64\n",
      " 5   Species        344 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()     # to get detailed info of the datasets like type of data and no of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b48db",
   "metadata": {},
   "source": [
    "## Example 2: Loading a massive amounts of data using chunksize argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Pranay/Downloads/penguins.csv\", chunksize=10000)\n",
    "print.print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a307f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in df:\n",
    "\tpprint(data.shape)\n",
    "                            #344 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the number of chunks-\n",
    "df = pd.read_csv(\"C:/Users/Pranay/Downloads/penguins.csv\", chunksize=10)\n",
    "\n",
    "for data in df:\n",
    "\tpprint(data)\n",
    "\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0805481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
