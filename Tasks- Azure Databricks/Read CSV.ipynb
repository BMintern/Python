{"cells":[{"cell_type":"code","source":["# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit\n# Initialize SparkSession\nspark = SparkSession.builder.appName(\"CSV to Parquet\").getOrCreate()\n\n# Step 1: Read CSV file into a DataFrame\ncsv_file_path = \"/FileStore/tables/Employees.csv\"\ndf = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n\n# Step 2: Add a new column to the DataFrame\n# For example, let's add a column called \"new_column\" with constant value \"new_value\"\nnew_value = \"Salary\"\ndf_with_new_column = df.withColumn(\"salary\", lit(500000))\n\n# Step 3: Apply filter on the DataFrame\n# For example, let's filter the DataFrame to keep only rows with a specific condition\nfiltered_df = df_with_new_column.filter(df_with_new_column[\"salary\"] > 400000)\n\n# Step 4: Save the DataFrame as Parquet format\noutput_parquet_path = \"/FileStore/tables/output_Employees.parquet\"\nfiltered_df.write.parquet(output_parquet_path.parquet)\n\n# Stop the SparkSession\nspark.stop()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ff41774a-4f52-43ca-88a6-d0e918c94ff6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession\nspark = SparkSession.builder.appName(\"Read Parquet\").getOrCreate()\n\n# Step 1: Read the Parquet file into a DataFrame\nparquet_file_path = \"/FileStore/tables/output_Employees.parquet\"\ndf = spark.read.parquet(parquet_file_path)\n\n# Step 2: Display the contents of the DataFrame\ndf.show()\n\n# Stop the SparkSession\n# spark.stop()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9bbe49a1-73e8-4e9c-8655-c639dc721181","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+-------+---------+------+\n|emp_id|emp_name| gender|  country|salary|\n+------+--------+-------+---------+------+\n|  1001|  Pranay|   Male|    India|500000|\n|  1002|Priyanka|Female |      USA|500000|\n|  1003|   Ayush|   Male|    India|500000|\n|  1004|  Ritika|Female |  England|500000|\n|  1005|     Sam|   Male|Australia|500000|\n+------+--------+-------+---------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc1dd8b7-a44c-4c5a-b97a-65a8bc6baa12","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Read CSV","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
